{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1782442,"sourceType":"datasetVersion","datasetId":1059701}],"dockerImageVersionId":31234,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%writefile models.py\n\"\"\"\nUnified Model Collection for PFN Experiments\nIncludes: PartitionedMLP, BottleneckMLP, DeepMLP, StandardCNN, BottleneckCNN, DeepCNN, PartitionedCNN\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nfrom typing import List, Dict\n\n\n# ==================== MLP MODELS ====================\n\nclass PartitionedLinear(nn.Module):\n    \"\"\"Linear layer partitioned into sub-blocks for PFN analysis.\"\"\"\n    \n    def __init__(self, in_features: int, out_features: int, num_blocks: int):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.num_blocks = num_blocks\n        self.block_size = out_features // num_blocks\n        \n        self.blocks = nn.ModuleList([\n            nn.Linear(in_features, self.block_size, bias=True)\n            for i in range(num_blocks)\n        ])\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return torch.cat([block(x) for block in self.blocks], dim=-1)\n    \n    def get_block_params(self, block_idx: int) -> List[nn.Parameter]:\n        return list(self.blocks[block_idx].parameters())\n    \n    def get_block_gradients(self, block_idx: int) -> torch.Tensor:\n        grads = [p.grad.flatten() for p in self.blocks[block_idx].parameters() if p.grad is not None]\n        if grads: return torch.cat(grads)\n        return torch.tensor([0.0], device=next(self.blocks[block_idx].parameters()).device)\n\n\nclass PartitionedMLP(nn.Module):\n    \"\"\"\n    Enhanced MLP for MNIST with sub-block partitioning and Dropout.\n    - Input: 784\n    - Hidden1: 512 (4 blocks of 128) -> ReLU -> Dropout\n    - Hidden2: 256 (2 blocks of 128) -> ReLU -> Dropout\n    - Output: 10\n    \"\"\"\n    \n    def __init__(self):\n        super().__init__()\n        self.layer1 = PartitionedLinear(784, 512, num_blocks=4)\n        self.dropout1 = nn.Dropout(0.2)\n        self.layer2 = PartitionedLinear(512, 256, num_blocks=2)\n        self.dropout2 = nn.Dropout(0.2)\n        self.layer3 = nn.Linear(256, 10)\n        self.relu = nn.ReLU()\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x.view(-1, 784)\n        x = self.dropout1(self.relu(self.layer1(x)))\n        x = self.dropout2(self.relu(self.layer2(x)))\n        return self.layer3(x)\n    \n    def get_all_block_gradients(self) -> Dict[str, List[torch.Tensor]]:\n        gradients = {}\n        device = next(self.parameters()).device\n        gradients['layer1'] = [self.layer1.get_block_gradients(i) for i in range(4)]\n        gradients['layer2'] = [self.layer2.get_block_gradients(i) for i in range(2)]\n        grads = [p.grad.flatten() for p in self.layer3.parameters() if p.grad is not None]\n        gradients['layer3'] = [torch.cat(grads)] if grads else [torch.tensor([0.0], device=device)]\n        return gradients\n    \n    def get_parameter_groups(self) -> List[Dict]:\n        groups = []\n        for i in range(4): groups.append({'params': self.layer1.get_block_params(i), 'name': f'layer1_block{i}', 'lr': 0.001})\n        for i in range(2): groups.append({'params': self.layer2.get_block_params(i), 'name': f'layer2_block{i}', 'lr': 0.001})\n        groups.append({'params': list(self.layer3.parameters()), 'name': 'layer3_block0', 'lr': 0.001})\n        return groups\n\n\nclass BottleneckMLP(nn.Module):\n    \"\"\"\n    Hourglass MLP with artificial bottleneck layers.\n    Architecture: 784 -> 256 -> 64 -> 8 -> 64 -> 256 -> 10\n    The narrow 8-neuron layer (reduced from 16) creates a strong information bottleneck.\n    \"\"\"\n    \n    def __init__(self, bottleneck_width: int = 8):\n        super().__init__()\n        self.enc1, self.enc2, self.enc3 = nn.Linear(784, 256), nn.Linear(256, 64), nn.Linear(64, bottleneck_width)\n        self.dec1, self.dec2, self.output = nn.Linear(bottleneck_width, 64), nn.Linear(64, 256), nn.Linear(256, 10)\n        self.relu, self.dropout = nn.ReLU(), nn.Dropout(0.1)\n        self.layers = [self.enc1, self.enc2, self.enc3, self.dec1, self.dec2, self.output]\n        self.layer_names = ['enc1', 'enc2', 'bottleneck', 'dec1', 'dec2', 'output']\n    \n    def forward(self, x):\n        x = x.view(-1, 784)\n        for i, layer in enumerate(self.layers[:-1]):\n            x = self.dropout(self.relu(layer(x))) if i != 2 else self.relu(layer(x))\n        return self.output(x)\n    \n    def get_all_block_gradients(self) -> Dict[str, List[torch.Tensor]]:\n        gradients = {}\n        device = next(self.parameters()).device\n        for name, layer in zip(self.layer_names, self.layers):\n            grads = [p.grad.flatten() for p in layer.parameters() if p.grad is not None]\n            gradients[name] = [torch.cat(grads)] if grads else [torch.tensor([0.0], device=device)]\n        return gradients\n    \n    def get_parameter_groups(self) -> List[Dict]:\n        return [{'params': list(layer.parameters()), 'name': f'{name}_block0', 'lr': 0.001}\n                for name, layer in zip(self.layer_names, self.layers)]\n\n\nclass DeepMLP(nn.Module):\n    \"\"\"\n    Very deep MLP (10+ layers) without residual connections.\n    This will suffer from gradient vanishing, perfect for PFN to help.\n    \"\"\"\n    \n    def __init__(self, num_hidden_layers: int = 10, hidden_dim: int = 128):\n        super().__init__()\n        layers = [nn.Linear(784, hidden_dim)] + [nn.Linear(hidden_dim, hidden_dim) for _ in range(num_hidden_layers - 1)] + [nn.Linear(hidden_dim, 10)]\n        self.layers = nn.ModuleList(layers)\n        self.relu = nn.ReLU()\n        self.layer_names = [f'hidden_{i}' for i in range(num_hidden_layers)] + ['output']\n    \n    def forward(self, x):\n        x = x.view(-1, 784)\n        for layer in self.layers[:-1]: x = self.relu(layer(x))\n        return self.layers[-1](x)\n    \n    def get_all_block_gradients(self) -> Dict[str, List[torch.Tensor]]:\n        gradients = {}\n        device = next(self.parameters()).device\n        for name, layer in zip(self.layer_names, self.layers):\n            grads = [p.grad.flatten() for p in layer.parameters() if p.grad is not None]\n            gradients[name] = [torch.cat(grads)] if grads else [torch.tensor([0.0], device=device)]\n        return gradients\n    \n    def get_parameter_groups(self) -> List[Dict]:\n        return [{'params': list(layer.parameters()), 'name': f'{name}_block0', 'lr': 0.001}\n                for name, layer in zip(self.layer_names, self.layers)]\n\n\n# ==================== CNN MODELS ====================\n\nclass ChannelPartitionedConv2d(nn.Module):\n    \"\"\"将卷积层沿输出通道拆分为多个独立Block，构建并行流路径\"\"\"\n    \n    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, \n                 stride: int = 1, padding: int = 0, num_blocks: int = 4):\n        super().__init__()\n        assert out_channels % num_blocks == 0\n        self.num_blocks = num_blocks\n        self.block_size = out_channels // num_blocks\n        self.blocks = nn.ModuleList([\n            nn.Conv2d(in_channels, self.block_size, kernel_size, stride, padding)\n            for _ in range(num_blocks)\n        ])\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return torch.cat([block(x) for block in self.blocks], dim=1)\n\n\nclass PartitionedCNN(nn.Module):\n    \"\"\"专为PFN设计的分块CNN，每层4个并行流管道\"\"\"\n    \n    def __init__(self, num_blocks: int = 4, num_classes: int = 10):\n        super().__init__()\n        self.num_blocks = num_blocks\n        self.conv1 = ChannelPartitionedConv2d(3, 64, 3, padding=1, num_blocks=num_blocks)\n        self.conv2 = ChannelPartitionedConv2d(64, 128, 3, padding=1, num_blocks=num_blocks)\n        self.conv3 = ChannelPartitionedConv2d(128, 128, 3, padding=1, num_blocks=num_blocks)\n        self.relu, self.pool, self.gap = nn.ReLU(), nn.MaxPool2d(2), nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(128, num_classes)\n        self.layer_names = ['conv1', 'conv2', 'conv3']\n        self._layers = [self.conv1, self.conv2, self.conv3]\n    \n    def forward(self, x):\n        x = self.pool(self.relu(self.conv1(x)))\n        x = self.pool(self.relu(self.conv2(x)))\n        x = self.pool(self.relu(self.conv3(x)))\n        return self.fc(self.gap(x).view(x.size(0), -1))\n    \n    def get_all_block_gradients(self) -> Dict[str, List[torch.Tensor]]:\n        gradients = {}\n        device = next(self.parameters()).device\n        for name, layer in zip(self.layer_names, self._layers):\n            block_grads = []\n            for block in layer.blocks:\n                grads = [p.grad.flatten() for p in block.parameters() if p.grad is not None]\n                block_grads.append(torch.cat(grads) if grads else torch.tensor([0.0], device=device))\n            gradients[name] = block_grads\n        fc_grads = [p.grad.flatten() for p in self.fc.parameters() if p.grad is not None]\n        gradients['fc'] = [torch.cat(fc_grads)] if fc_grads else [torch.tensor([0.0], device=device)]\n        return gradients\n    \n    def get_parameter_groups(self) -> List[Dict]:\n        groups = []\n        for name, layer in zip(self.layer_names, self._layers):\n            for i, block in enumerate(layer.blocks):\n                groups.append({'params': list(block.parameters()), 'name': f'{name}_block{i}', 'lr': 0.001})\n        groups.append({'params': list(self.fc.parameters()), 'name': 'fc_block0', 'lr': 0.001})\n        return groups\n\n\nclass DeepPartitionedCNN(nn.Module):\n    \"\"\"\n    真正的PFN主场：既深（梯度消失），又宽（分块路由）。\n    无残差连接，无BatchNorm，制造梯度消失瓶颈让PFN发挥作用。\n    \"\"\"\n    \n    def __init__(self, num_layers: int = 12, num_classes: int = 10, num_blocks: int = 4):\n        super().__init__()\n        self.num_layers = num_layers\n        self.num_blocks = num_blocks\n        \n        # 初始层\n        self.conv_in = ChannelPartitionedConv2d(3, 32, 3, padding=1, num_blocks=num_blocks)\n        \n        # 中间深层\n        self.hidden_layers = nn.ModuleList()\n        \n        in_ch, out_ch = 32, 32\n        for i in range(num_layers - 1):\n            if i > 0 and i % 4 == 0:\n                out_ch = min(in_ch * 2, 128)\n            self.hidden_layers.append(ChannelPartitionedConv2d(in_ch, out_ch, 3, padding=1, num_blocks=num_blocks))\n            in_ch = out_ch\n        \n        self.pool = nn.AdaptiveAvgPool2d(1)\n        self.classifier = nn.Linear(out_ch, num_classes)\n        self.relu = nn.ReLU()\n        self.maxpool = nn.MaxPool2d(2)\n        \n        self.layer_names = ['conv_in'] + [f'conv_{i}' for i in range(len(self.hidden_layers))] + ['classifier']\n    \n    def forward(self, x):\n        x = self.relu(self.conv_in(x))\n        \n        for i, conv in enumerate(self.hidden_layers):\n            x = self.relu(conv(x))\n            if i > 0 and i % 4 == 0:\n                x = self.maxpool(x)\n        \n        x = self.pool(x)\n        return self.classifier(x.view(x.size(0), -1))\n    \n    def get_all_block_gradients(self) -> Dict[str, List[torch.Tensor]]:\n        gradients = {}\n        device = next(self.parameters()).device\n        \n        # conv_in\n        block_grads = []\n        for block in self.conv_in.blocks:\n            grads = [p.grad.flatten() for p in block.parameters() if p.grad is not None]\n            block_grads.append(torch.cat(grads) if grads else torch.tensor([0.0], device=device))\n        gradients['conv_in'] = block_grads\n        \n        # 中间层\n        for i, layer in enumerate(self.hidden_layers):\n            block_grads = []\n            for block in layer.blocks:\n                grads = [p.grad.flatten() for p in block.parameters() if p.grad is not None]\n                block_grads.append(torch.cat(grads) if grads else torch.tensor([0.0], device=device))\n            gradients[f'conv_{i}'] = block_grads\n        \n        # Classifier\n        fc_grads = [p.grad.flatten() for p in self.classifier.parameters() if p.grad is not None]\n        gradients['classifier'] = [torch.cat(fc_grads)] if fc_grads else [torch.tensor([0.0], device=device)]\n        \n        return gradients\n    \n    def get_parameter_groups(self) -> List[Dict]:\n        groups = []\n        \n        # conv_in\n        for i, block in enumerate(self.conv_in.blocks):\n            groups.append({'params': list(block.parameters()), 'name': f'conv_in_block{i}', 'lr': 0.001})\n        \n        # 中间层\n        for layer_idx, layer in enumerate(self.hidden_layers):\n            for i, block in enumerate(layer.blocks):\n                groups.append({'params': list(block.parameters()), 'name': f'conv_{layer_idx}_block{i}', 'lr': 0.001})\n        \n        groups.append({'params': list(self.classifier.parameters()), 'name': 'classifier_block0', 'lr': 0.001})\n        \n        return groups\n\n\nclass BottleneckCNN(nn.Module):\n    \"\"\"\n    CNN with strong bottleneck architecture for CIFAR-10.\n    Creates artificial information bottleneck to test PFN.\n    No BatchNorm, reduced bottleneck width (4 instead of 8).\n    \"\"\"\n    \n    def __init__(self, bottleneck_channels: int = 4, num_classes: int = 10):\n        super().__init__()\n        self.enc1 = nn.Sequential(nn.Conv2d(3, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2))\n        self.enc2 = nn.Sequential(nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2))\n        self.bottleneck = nn.Sequential(nn.Conv2d(128, bottleneck_channels, 1), nn.ReLU())\n        self.dec1 = nn.Sequential(nn.Conv2d(bottleneck_channels, 128, 3, padding=1), nn.ReLU())\n        self.dec2 = nn.Sequential(nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(), nn.AdaptiveAvgPool2d(1))\n        self.classifier = nn.Linear(256, num_classes)\n        self.layer_names = ['enc1', 'enc2', 'bottleneck', 'dec1', 'dec2', 'classifier']\n        self._layers = [self.enc1, self.enc2, self.bottleneck, self.dec1, self.dec2, self.classifier]\n    \n    def forward(self, x):\n        x = self.dec2(self.dec1(self.bottleneck(self.enc2(self.enc1(x)))))\n        return self.classifier(x.view(x.size(0), -1))\n    \n    def get_all_block_gradients(self) -> Dict[str, List[torch.Tensor]]:\n        gradients = {}\n        device = next(self.parameters()).device\n        for name, layer in zip(self.layer_names, self._layers):\n            grads = [p.grad.flatten() for p in layer.parameters() if p.grad is not None]\n            gradients[name] = [torch.cat(grads)] if grads else [torch.tensor([0.0], device=device)]\n        return gradients\n    \n    def get_parameter_groups(self) -> List[Dict]:\n        return [{'params': list(layer.parameters()), 'name': f'{name}_block0', 'lr': 0.001}\n                for name, layer in zip(self.layer_names, self._layers)]\n\n\n# 新增：VGG 相关工具（不使用 BatchNorm）\ndef make_vgg_layers(cfg: List, batch_norm: bool = False):\n\tlayers = []\n\tin_channels = 3\n\tfor v in cfg:\n\t\tif v == 'M':\n\t\t\tlayers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n\t\telse:\n\t\t\tconv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n\t\t\tif batch_norm:\n\t\t\t\tlayers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n\t\t\telse:\n\t\t\t\tlayers += [conv2d, nn.ReLU(inplace=True)]\n\t\t\tin_channels = v\n\treturn nn.Sequential(*layers)\n\nclass VGG11NoBN(nn.Module):\n\t\"\"\"VGG-11 without BatchNorm，适配 CIFAR（AdaptiveAvgPool2d -> Linear）\"\"\"\n\tdef __init__(self, num_classes: int = 10):\n\t\tsuper().__init__()\n\t\t# cfg 'A' 对应 VGG11（no BatchNorm）\n\t\tcfg_A = [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']\n\t\tself.features = make_vgg_layers(cfg_A, batch_norm=False)\n\t\tself.pool = nn.AdaptiveAvgPool2d(1)\n\t\tself.classifier = nn.Linear(512, num_classes)\n\n\tdef forward(self, x: torch.Tensor) -> torch.Tensor:\n\t\tx = self.features(x)\n\t\tx = self.pool(x).view(x.size(0), -1)\n\t\treturn self.classifier(x)\n\n\tdef get_all_block_gradients(self) -> Dict[str, List[torch.Tensor]]:\n\t\tgradients = {}\n\t\tdevice = next(self.parameters()).device\n\t\tconv_idx = 0\n\t\tfor m in self.features:\n\t\t\tif isinstance(m, nn.Conv2d):\n\t\t\t\tgrads = [p.grad.flatten() for p in m.parameters() if p.grad is not None]\n\t\t\t\tgradients[f'conv{conv_idx}'] = [torch.cat(grads)] if grads else [torch.tensor([0.0], device=device)]\n\t\t\t\tconv_idx += 1\n\t\tfc_grads = [p.grad.flatten() for p in self.classifier.parameters() if p.grad is not None]\n\t\tgradients['classifier'] = [torch.cat(fc_grads)] if fc_grads else [torch.tensor([0.0], device=device)]\n\t\treturn gradients\n\n\tdef get_parameter_groups(self) -> List[Dict]:\n\t\tgroups = []\n\t\tconv_idx = 0\n\t\tfor m in self.features:\n\t\t\tif isinstance(m, nn.Conv2d):\n\t\t\t\tgroups.append({'params': list(m.parameters()), 'name': f'conv{conv_idx}_block0', 'lr': 0.001})\n\t\t\t\tconv_idx += 1\n\t\tgroups.append({'params': list(self.classifier.parameters()), 'name': 'classifier_block0', 'lr': 0.001})\n\t\treturn groups\n\n\n# ==================== MODEL FACTORY ====================\n\ndef get_model(scenario: str, dataset: str, **kwargs) -> nn.Module:\n\tnum_classes = 100 if dataset == 'cifar100' else 10\n\t\n\tif dataset == 'mnist':\n\t\tif scenario == 'standard': return PartitionedMLP()\n\t\telif scenario == 'bottleneck': return BottleneckMLP(bottleneck_width=kwargs.get('bottleneck_width', 8))\n\t\telif scenario == 'deep': return DeepMLP(num_hidden_layers=kwargs.get('num_layers', 10))\n\t\n\telif dataset in ['cifar10', 'cifar100']:\n\t\t# 标准场景使用无 BatchNorm 的 VGG-11\n\t\tif scenario == 'standard':\n\t\t\treturn VGG11NoBN(num_classes=num_classes)\n\t\telif scenario == 'bottleneck': \n\t\t\treturn BottleneckCNN(bottleneck_channels=kwargs.get('bottleneck_width', 4), num_classes=num_classes)\n\t\telif scenario == 'deep': \n\t\t\treturn DeepPartitionedCNN(num_layers=kwargs.get('num_layers', 12), num_classes=num_classes, num_blocks=4)\n\t\n\traise ValueError(f\"Unknown scenario '{scenario}' or dataset '{dataset}'\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-05T06:19:09.818240Z","iopub.execute_input":"2026-01-05T06:19:09.818682Z","iopub.status.idle":"2026-01-05T06:19:09.831296Z","shell.execute_reply.started":"2026-01-05T06:19:09.818644Z","shell.execute_reply":"2026-01-05T06:19:09.830560Z"}},"outputs":[{"name":"stdout","text":"Overwriting models.py\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"%%writefile pfn.py\n\"\"\"\nParameter Flow Network (PFN) - 极简物理版\n核心原则：容量 = 梯度能量，不要玄学\n\"\"\"\n\nimport torch\nimport numpy as np\nfrom torch.optim import Optimizer\nfrom collections import deque, defaultdict\nfrom typing import Dict, List, Set, Tuple, Optional\n\n\nclass PFNGraphBuilder:\n    \"\"\"极简图构建器：容量 = 归一化梯度范数 × 深度补偿\"\"\"\n    \n    def __init__(self, use_hessian_approx: bool = False, depth_penalty: bool = True, history_size: int = 5):\n        self.num_nodes = 0\n        self.source = 0\n        self.sink = 0\n        self.node_map = {}\n        self.node_names = {}\n        self.layer_names = []\n        self.current_epoch = 0\n        self.total_epochs = 1\n        self.history_size = history_size\n        self.grad_history: List[Dict[str, List[float]]] = []\n        self.debug = True  # 开启调试输出\n    \n    def setup_topology(self, gradients: Dict[str, List[torch.Tensor]]):\n        self.layer_names = list(gradients.keys())\n        self.node_map = {}\n        self.node_names = {}\n        \n        current_id = 1\n        for layer_name in self.layer_names:\n            for block_idx in range(len(gradients[layer_name])):\n                self.node_map[(layer_name, block_idx)] = current_id\n                self.node_names[current_id] = f\"{layer_name}_block{block_idx}\"\n                current_id += 1\n        \n        self.source = 0\n        self.sink = current_id\n        self.num_nodes = current_id + 1\n    \n    def _compute_normalized_energy(self, gradients: Dict[str, List[torch.Tensor]]) -> Dict[str, List[float]]:\n        \"\"\"计算归一化的梯度能量\"\"\"\n        # 收集所有范数\n        all_norms = []\n        raw_norms = {}\n        \n        for layer_name, blocks in gradients.items():\n            raw_norms[layer_name] = []\n            for grad in blocks:\n                norm = grad.norm(2).item()\n                raw_norms[layer_name].append(norm)\n                all_norms.append(norm)\n        \n        # 计算平均范数用于归一化\n        avg_norm = np.mean(all_norms) + 1e-9\n        \n        # 归一化\n        normalized = {}\n        for layer_name, norms in raw_norms.items():\n            normalized[layer_name] = [n / avg_norm for n in norms]\n        \n        return normalized\n    \n    def _get_smoothed_energy(self, current_energy: Dict[str, List[float]]) -> Dict[str, List[float]]:\n        \"\"\"使用历史平滑能量值，减少噪声\"\"\"\n        self.grad_history.append(current_energy)\n        if len(self.grad_history) > self.history_size:\n            self.grad_history.pop(0)\n        \n        if len(self.grad_history) == 1:\n            return current_energy\n        \n        smoothed = {}\n        for layer_name in current_energy:\n            smoothed[layer_name] = []\n            for b_idx in range(len(current_energy[layer_name])):\n                values = [h[layer_name][b_idx] for h in self.grad_history if layer_name in h and b_idx < len(h[layer_name])]\n                smoothed[layer_name].append(np.mean(values) if values else current_energy[layer_name][b_idx])\n        \n        return smoothed\n    \n    def build_graph(self, gradients: Dict[str, List[torch.Tensor]]) -> Tuple[np.ndarray, Dict]:\n        \"\"\"构建流网络：容量 = 归一化梯度能量 × 深度补偿\"\"\"\n        self.setup_topology(gradients)\n        capacity = np.zeros((self.num_nodes, self.num_nodes))\n        \n        # 归一化并平滑\n        normalized_energy = self._compute_normalized_energy(gradients)\n        smoothed_energy = self._get_smoothed_energy(normalized_energy)\n        \n        num_layers = len(self.layer_names)\n        \n        for layer_idx, layer_name in enumerate(self.layer_names):\n            energies = smoothed_energy[layer_name]\n            # 深度补偿：更温和的指数补偿\n            depth_scale = 1.0 + 0.2 * layer_idx\n            \n            for b_idx, energy in enumerate(energies):\n                u = self.node_map[(layer_name, b_idx)]\n                scaled_energy = max(energy * depth_scale, 0.01)  # 最小容量提高到0.01\n                \n                # 1. 注入边：只有第一层连Source\n                if layer_idx == 0:\n                    capacity[self.source][u] = scaled_energy\n                \n                # 2. 传输边：Mesh Connectivity（交叉连接）\n                if layer_idx < num_layers - 1:\n                    next_layer = self.layer_names[layer_idx + 1]\n                    next_energies = smoothed_energy[next_layer]\n                    num_next_blocks = len(next_energies)\n                    \n                    # 连接到下一层的多个block（mesh connectivity）\n                    for offset in [-1, 0, 1]:\n                        nb_idx = b_idx + offset\n                        if 0 <= nb_idx < num_next_blocks:\n                            v = self.node_map[(next_layer, nb_idx)]\n                            # 直连权重1.0，邻近权重0.3\n                            weight = 1.0 if offset == 0 else 0.3\n                            capacity[u][v] = scaled_energy * weight\n                \n                # 3. 提取边：最后一层连Sink\n                if layer_idx == num_layers - 1:\n                    capacity[u][self.sink] = scaled_energy\n        \n        return capacity, {'normalized_energy': smoothed_energy}\n    \n    def get_node_name(self, node_idx: int) -> str:\n        if node_idx == self.source: return \"Source\"\n        if node_idx == self.sink: return \"Sink\"\n        return self.node_names.get(node_idx, f\"Node_{node_idx}\")\n\n\nclass IncrementalPushRelabel:\n    \"\"\"简化的Push-Relabel最大流算法\"\"\"\n    \n    def __init__(self):\n        self.flow = defaultdict(float)\n        self.height = {}\n        self.excess = {}\n        self.n = 0\n    \n    def find_min_cut(self, capacity: np.ndarray, source: int, sink: int) -> Tuple[float, List, Set, Set]:\n        self.n = capacity.shape[0]\n        max_flow = self._solve(capacity, source, sink)\n        \n        # 计算S集合（可达集）\n        S_set = {source}\n        queue = deque([source])\n        while queue:\n            u = queue.popleft()\n            for v in range(self.n):\n                if v not in S_set:\n                    residual = capacity[u][v] - self.flow.get((u, v), 0)\n                    if residual > 1e-9:\n                        S_set.add(v)\n                        queue.append(v)\n        \n        T_set = set(range(self.n)) - S_set\n        cut_edges = [(u, v) for u in S_set for v in T_set if capacity[u][v] > 1e-9]\n        \n        return max_flow, cut_edges, S_set, T_set\n    \n    def _solve(self, capacity: np.ndarray, source: int, sink: int) -> float:\n        n = self.n\n        self.height = {i: 0 for i in range(n)}\n        self.height[source] = n\n        self.excess = {i: 0.0 for i in range(n)}\n        self.flow = defaultdict(float)\n        \n        # 预流\n        for v in range(n):\n            if capacity[source][v] > 1e-9:\n                f = capacity[source][v]\n                self.flow[(source, v)] = f\n                self.flow[(v, source)] = -f\n                self.excess[v] = f\n                self.excess[source] -= f\n        \n        active = deque([v for v in range(n) if v != source and v != sink and self.excess[v] > 1e-9])\n        max_iter = n * n * 2\n        \n        for _ in range(max_iter):\n            if not active: break\n            u = active.popleft()\n            if self.excess[u] <= 1e-9: continue\n            \n            # Push\n            for v in range(n):\n                if self.excess[u] <= 1e-9: break\n                res = capacity[u][v] - self.flow.get((u, v), 0)\n                if res > 1e-9 and self.height[u] == self.height[v] + 1:\n                    delta = min(self.excess[u], res)\n                    self.flow[(u, v)] += delta\n                    self.flow[(v, u)] -= delta\n                    self.excess[u] -= delta\n                    self.excess[v] += delta\n                    if v != source and v != sink and self.excess[v] > 1e-9 and v not in active:\n                        active.append(v)\n            \n            # Relabel\n            if self.excess[u] > 1e-9:\n                min_h = float('inf')\n                for v in range(n):\n                    if capacity[u][v] - self.flow.get((u, v), 0) > 1e-9:\n                        min_h = min(min_h, self.height[v])\n                if min_h < float('inf'):\n                    self.height[u] = min_h + 1\n                    active.append(u)\n        \n        return max(self.excess.get(sink, 0), 0)\n\n\nDinicSolver = IncrementalPushRelabel\n\n\nclass BottleneckOptimizer:\n    \"\"\"温和的瓶颈补偿优化器 - 不重置LR，基于当前值调整\"\"\"\n    \n    def __init__(self, optimizer: Optimizer, base_lr: float = 0.001,\n                 base_boost: float = 1.3, max_boost: float = 3.0, decay_factor: float = 0.95):\n        self.optimizer = optimizer\n        self.base_lr = base_lr\n        self.base_boost = base_boost\n        self.max_boost = max_boost\n        self.decay_factor = decay_factor\n        self.name_to_idx = {g['name']: i for i, g in enumerate(optimizer.param_groups) if 'name' in g}\n        self.bottleneck_history: List[List[str]] = []\n        self.boost_counts: Dict[str, int] = defaultdict(int)\n        self.debug = True\n        self.step_count = 0\n    \n    def _node_to_param_group(self, node_name: str) -> Optional[str]:\n        if node_name in self.name_to_idx:\n            return node_name\n        for name in self.name_to_idx:\n            if node_name in name:\n                return name\n        return None\n    \n    def update_learning_rates(self, S_set: Set[int], T_set: Set[int],\n                              cut_edges: List[Tuple[int, int]],\n                              capacity_matrix: np.ndarray,\n                              graph_builder, flow_deficit: float = 0.0):\n        self.step_count += 1\n        \n        # Debug输出\n        if self.debug and self.step_count % 10 == 0:\n            total_potential = np.sum(capacity_matrix[graph_builder.source, :])\n            print(f\"\\n[PFN Debug] Step {self.step_count}\")\n            print(f\"  Total Potential Flow: {total_potential:.4f}\")\n            print(f\"  Graph Partition: S={len(S_set)}, T={len(T_set)}\")\n            \n            bottleneck_names = []\n            for u, v in cut_edges[:5]:  # 只显示前5个\n                u_name = graph_builder.get_node_name(u)\n                v_name = graph_builder.get_node_name(v)\n                bottleneck_names.append(f\"{u_name}->{v_name}\")\n            print(f\"  Cut Edges: {bottleneck_names}\")\n        \n        # 温和衰减所有非BN层的LR（不是重置！）\n        for group in self.optimizer.param_groups:\n            if 'name' in group and 'bn' not in group['name'].lower():\n                # 温和衰减，而不是重置\n                current_lr = group['lr']\n                target_lr = self.base_lr\n                # 缓慢向base_lr回归\n                group['lr'] = current_lr * self.decay_factor + target_lr * (1 - self.decay_factor)\n        \n        # 识别瓶颈节点\n        bottleneck_nodes = set()\n        for u, v in cut_edges:\n            if v != graph_builder.sink:\n                bottleneck_nodes.add(v)\n            if u != graph_builder.source:\n                bottleneck_nodes.add(u)\n        \n        # 基于当前LR进行boost（不是重置后boost）\n        boosted_names = []\n        for node_id in bottleneck_nodes:\n            node_name = graph_builder.get_node_name(node_id)\n            param_name = self._node_to_param_group(node_name)\n            \n            if param_name and param_name in self.name_to_idx:\n                idx = self.name_to_idx[param_name]\n                current_lr = self.optimizer.param_groups[idx]['lr']\n                \n                # 计算boost，考虑flow_deficit\n                boost = self.base_boost * (1.0 + min(flow_deficit, 0.5))\n                new_lr = min(current_lr * boost, self.base_lr * self.max_boost)\n                \n                self.optimizer.param_groups[idx]['lr'] = new_lr\n                self.boost_counts[param_name] += 1\n                boosted_names.append(f\"{param_name}({new_lr:.5f})\")\n        \n        if self.debug and self.step_count % 10 == 0 and boosted_names:\n            print(f\"  Boosted: {boosted_names[:3]}\")\n        \n        self.bottleneck_history.append([f\"{graph_builder.get_node_name(u)}->{graph_builder.get_node_name(v)}\" for u, v in cut_edges])\n    \n    def apply_gradient_clipping(self, model, T_set: Set[int], graph_builder):\n        pass\n    \n    def step(self):\n        self.optimizer.step()\n    \n    def zero_grad(self):\n        self.optimizer.zero_grad()\n    \n    def get_statistics(self) -> Dict:\n        return {\n            'bottleneck_history': self.bottleneck_history,\n            'boost_counts': dict(self.boost_counts)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T06:19:09.832798Z","iopub.execute_input":"2026-01-05T06:19:09.833078Z","iopub.status.idle":"2026-01-05T06:19:09.851946Z","shell.execute_reply.started":"2026-01-05T06:19:09.833043Z","shell.execute_reply":"2026-01-05T06:19:09.851269Z"}},"outputs":[{"name":"stdout","text":"Overwriting pfn.py\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"%%writefile run_experiment.py\n\"\"\"PFN Experiment - Parameter Flow Network for Neural Network Optimization.\"\"\"\n\nimport os, json, argparse, torch, numpy as np\nimport torch.nn as nn, torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import datasets, transforms\nfrom tqdm import tqdm\nfrom datetime import datetime\nfrom typing import Dict, List, Optional\nfrom models import get_model\nfrom pfn import PFNGraphBuilder, IncrementalPushRelabel, BottleneckOptimizer\n\n\n# ==================== CONFIG ====================\nCONFIG = {\n    'mnist': {\n        'epochs_list': [30],\n        'batches_list': [32],\n        'scenarios': [\n            {'name': '0.Original', 'scenario': 'standard'},\n            {'name': '1.Bottleneck', 'scenario': 'bottleneck', 'bottleneck_width': 4},\n            {'name': '2.Deep', 'scenario': 'deep', 'num_layers': 15, 'lr': 0.0005},\n            {'name': '3.Noisy', 'scenario': 'standard', 'pixel_noise': 0.3, 'label_noise': 0.15, 'samples': 100},\n        ]\n    },\n    'cifar10': {\n        'epochs_list': [50],\n        'batches_list': [64,128,256,512],\n        'scenarios': [\n            {'name': '0.Original', 'scenario': 'standard'},\n            {'name': '1.Bottleneck', 'scenario': 'bottleneck', 'bottleneck_width': 2},\n            {'name': '2.Deep', 'scenario': 'deep', 'num_layers': 15, 'lr': 0.0005},\n            {'name': '3.Noisy', 'scenario': 'standard', 'pixel_noise': 0.2, 'label_noise': 0.1, 'samples': 200},\n        ]\n    },\n    'cifar100': {\n        'epochs_list': [100],\n        'batches_list': [1024],\n        'scenarios': [\n            {'name': '1.Bottleneck', 'scenario': 'bottleneck', 'bottleneck_width': 2},\n            {'name': '2.Deep', 'scenario': 'deep', 'num_layers': 15, 'lr': 0.0005},\n            {'name': '3.Noisy', 'scenario': 'standard', 'pixel_noise': 0.15, 'label_noise': 0.1, 'samples': 300},\n        ]\n    },\n}\nPFN_INTERVAL = 100  # 增加间隔，减少干扰Adam\nPFN_WARMUP_STEPS = 200  # 前200步不介入，让Adam稳定\nBASE_LR = 0.0001\n\n\n# ==================== BASELINE CACHE ====================\nclass BaselineCache:\n    \"\"\"Baseline结果缓存系统\"\"\"\n    \n    def __init__(self, cache_dir: str = './results-baseline'):\n        self.cache_dir = cache_dir\n        os.makedirs(cache_dir, exist_ok=True)\n    \n    def _get_cache_key(self, dataset: str, scenario: str, batch_size: int, epochs: int) -> str:\n        \"\"\"生成缓存键\"\"\"\n        return f\"{dataset}_{scenario}_b{batch_size}_e{epochs}\"\n    \n    def get(self, dataset: str, scenario: str, batch_size: int, epochs: int) -> Optional[Dict]:\n        \"\"\"获取缓存的baseline结果\"\"\"\n        key = self._get_cache_key(dataset, scenario, batch_size, epochs)\n        cache_file = os.path.join(self.cache_dir, f\"{key}.json\")\n        \n        if os.path.exists(cache_file):\n            try:\n                with open(cache_file, 'r') as f:\n                    return json.load(f)\n            except:\n                return None\n        return None\n    \n    def save(self, dataset: str, scenario: str, batch_size: int, epochs: int, \n             acc: float, loss_hist: List[float], acc_hist: List[float]):\n        \"\"\"保存baseline结果\"\"\"\n        key = self._get_cache_key(dataset, scenario, batch_size, epochs)\n        cache_file = os.path.join(self.cache_dir, f\"{key}.json\")\n        \n        data = {\n            'acc': float(acc),\n            'loss': loss_hist,\n            'acc_hist': acc_hist,\n            'timestamp': datetime.now().isoformat()\n        }\n        \n        with open(cache_file, 'w') as f:\n            json.dump(data, f)\n\n\nbaseline_cache = BaselineCache()\n\n\n# ==================== DATA ====================\nclass NoisyDataset(Dataset):\n    \"\"\"Dataset wrapper with pixel and label noise.\"\"\"\n    def __init__(self, base_dataset, pixel_noise=0.0, label_noise=0.0, num_classes=10):\n        self.base = base_dataset\n        self.pixel_noise = pixel_noise\n        self.num_classes = num_classes\n        \n        self.noisy_labels = []\n        np.random.seed(42)\n        for i in range(len(base_dataset)):\n            _, label = base_dataset[i]\n            label = label.item() if isinstance(label, torch.Tensor) else label\n            if label_noise > 0 and np.random.random() < label_noise:\n                candidates = [l for l in range(num_classes) if l != label]\n                label = np.random.choice(candidates)\n            self.noisy_labels.append(label)\n    \n    def __len__(self): return len(self.base)\n    \n    def __getitem__(self, idx):\n        img, _ = self.base[idx]\n        label = self.noisy_labels[idx]\n        if self.pixel_noise > 0:\n            img = img + torch.randn_like(img) * self.pixel_noise\n        return img, label\n\n\nclass SmallDataset(Dataset):\n    \"\"\"Dataset wrapper that limits samples per class.\"\"\"\n    def __init__(self, base_dataset, samples_per_class, num_classes=10):\n        self.base = base_dataset\n        indices_per_class = {c: [] for c in range(num_classes)}\n        for idx in range(len(base_dataset)):\n            _, label = base_dataset[idx]\n            label = label.item() if isinstance(label, torch.Tensor) else label\n            if len(indices_per_class[label]) < samples_per_class:\n                indices_per_class[label].append(idx)\n        self.indices = [i for c in range(num_classes) for i in indices_per_class[c]]\n    \n    def __len__(self): return len(self.indices)\n    def __getitem__(self, idx): return self.base[self.indices[idx]]\n\n\ndef get_loaders(dataset_name, batch_size, pixel_noise=0, label_noise=0, samples_per_class=None):\n    num_classes = 100 if dataset_name == 'cifar100' else 10\n    \n    if dataset_name == 'mnist':\n        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n        train_ds = datasets.MNIST('./data', train=True, download=True, transform=transform)\n        test_ds = datasets.MNIST('./data', train=False, transform=transform)\n    else:\n        train_transform = transforms.Compose([\n            transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n        ])\n        test_transform = transforms.Compose([\n            transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n        ])\n        \n        # --- 针对 Kaggle 路径适配 ---\n        if dataset_name == 'cifar100' and os.path.exists('/kaggle/input/cifar100'):\n            # 创建虚拟目录结构，使之符合 torchvision 期待的 'cifar-100-python' 命名\n            tmp_root = './tmp_cifar'\n            os.makedirs(tmp_root, exist_ok=True)\n            target_link = os.path.join(tmp_root, 'cifar-100-python')\n            if not os.path.exists(target_link):\n                os.symlink('/kaggle/input/cifar100', target_link)\n            \n            train_ds = datasets.CIFAR100(root=tmp_root, train=True, download=False, transform=train_transform)\n            test_ds = datasets.CIFAR100(root=tmp_root, train=False, download=False, transform=test_transform)\n        elif dataset_name == 'cifar100':\n            train_ds = datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transform)\n            test_ds = datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)\n        else:\n            # CIFAR10 逻辑\n            train_ds = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n            test_ds = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n    \n    # Apply modifications\n    if samples_per_class:\n        train_ds = SmallDataset(train_ds, samples_per_class, num_classes)\n    if pixel_noise > 0 or label_noise > 0:\n        train_ds = NoisyDataset(train_ds, pixel_noise, label_noise, num_classes)\n    \n    train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n    test_loader = DataLoader(test_ds, batch_size * 2, shuffle=False, num_workers=2)\n    return train_loader, test_loader\n\n\n# ==================== DEVICE ====================\ndef get_device(force_cpu=False):\n    if force_cpu: return torch.device('cpu')\n    if torch.cuda.is_available(): return torch.device('cuda')\n    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available(): return torch.device('mps')\n    return torch.device('cpu')\n\n\n# ==================== TRAINING ====================\ndef train_epoch(model, loader, opt, criterion, device, pfn=None, step=0, epoch=0, total_epochs=1):\n    model.train()\n    total_loss = 0\n    \n    for x, y in tqdm(loader, desc=\"    train\", leave=False):\n        x, y = x.to(device), y.to(device)\n        \n        (pfn['opt'] if pfn else opt).zero_grad()\n        loss = criterion(model(x), y)\n        loss.backward()\n        \n        if pfn and step > PFN_WARMUP_STEPS and step % PFN_INTERVAL == 0:\n            grads = {k: [t.detach().cpu() for t in v] for k, v in model.get_all_block_gradients().items()}\n            pfn['gb'].current_epoch = epoch\n            pfn['gb'].total_epochs = total_epochs\n            \n            cap, meta = pfn['gb'].build_graph(grads)\n            max_flow, cuts, S, T = pfn['solver'].find_min_cut(cap, pfn['gb'].source, pfn['gb'].sink)\n            \n            total_cap = sum(cap[u][v] for u, v in cuts) if cuts else 1.0\n            flow_deficit = (total_cap - max_flow) / (total_cap + 1e-9)\n            pfn['opt'].update_learning_rates(S, T, cuts, cap, pfn['gb'], flow_deficit)\n        \n        (pfn['opt'] if pfn else opt).step()\n        total_loss += loss.item()\n        step += 1\n    \n    return total_loss / len(loader), step\n\n\ndef evaluate(model, loader, device):\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for x, y in loader:\n            x, y = x.to(device), y.to(device)\n            correct += (model(x).argmax(1) == y).sum().item()\n            total += y.size(0)\n    return correct / total\n\n\ndef train_model(config: dict, device, use_pfn=False):\n    lr = config.get('lr', BASE_LR)\n    train_loader, test_loader = get_loaders(\n        config['dataset'], config['batch_size'],\n        config.get('pixel_noise', 0), config.get('label_noise', 0), config.get('samples')\n    )\n    \n    model = get_model(\n        config['scenario'], config['dataset'],\n        bottleneck_width=config.get('bottleneck_width', 8),\n        num_layers=config.get('num_layers', 10)\n    ).to(device)\n    \n    if use_pfn:\n        groups = model.get_parameter_groups()\n        for g in groups: g['lr'] = lr\n        opt = optim.Adam(groups, lr=lr)\n        pfn = {\n            'gb': PFNGraphBuilder(history_size=5),\n            'solver': IncrementalPushRelabel(),\n            'opt': BottleneckOptimizer(opt, lr, base_boost=1.3, max_boost=3.0, decay_factor=0.95)\n        }\n        pfn['gb'].debug = (config['epochs'] <= 15)\n        pfn['opt'].debug = (config['epochs'] <= 15)\n    else:\n        opt = optim.Adam(model.parameters(), lr=lr)\n        pfn = None\n    \n    scheduler = optim.lr_scheduler.CosineAnnealingLR(opt, config['epochs'])\n    step = 0\n    loss_hist, acc_hist = [], []\n    \n    for ep in range(config['epochs']):\n        loss, step = train_epoch(model, train_loader, opt, nn.CrossEntropyLoss(), device, pfn, step, ep, config['epochs'])\n        scheduler.step()\n        acc = evaluate(model, test_loader, device)\n        loss_hist.append(loss)\n        acc_hist.append(acc)\n        print(f\"    [{ep+1:2d}/{config['epochs']}] loss={loss:.4f} acc={acc:.4f}\")\n    \n    return acc, loss_hist, acc_hist\n\n\ndef run_scenario(dataset, cfg, epochs, batch, device, results_dir):\n    name = cfg['name']\n    os.makedirs(os.path.join(results_dir, name), exist_ok=True)\n    \n    config = {\n        'dataset': dataset, 'scenario': cfg['scenario'],\n        'epochs': cfg.get('epochs', epochs), 'batch_size': cfg.get('batch_size', batch),\n        'lr': cfg.get('lr', BASE_LR), 'bottleneck_width': cfg.get('bottleneck_width', 8),\n        'num_layers': cfg.get('num_layers', 10), 'pixel_noise': cfg.get('pixel_noise', 0),\n        'label_noise': cfg.get('label_noise', 0), 'samples': cfg.get('samples')\n    }\n    \n    print(f\"  [Baseline]\", end=' ')\n    cached = baseline_cache.get(dataset, cfg['scenario'], config['batch_size'], config['epochs'])\n    \n    if cached:\n        print(\"(缓存)\")\n        base_acc = cached['acc']\n        base_loss = cached['loss']\n        base_acc_hist = cached['acc_hist']\n    else:\n        print(\"(训练)\")\n        base_acc, base_loss, base_acc_hist = train_model(config, device, False)\n        baseline_cache.save(dataset, cfg['scenario'], config['batch_size'], config['epochs'],\n                           base_acc, base_loss, base_acc_hist)\n    \n    print(f\"  [PFN]\")\n    pfn_acc, pfn_loss, pfn_acc_hist = train_model(config, device, True)\n    \n    with open(os.path.join(results_dir, name, 'log.json'), 'w') as f:\n        json.dump({'baseline': {'loss': base_loss, 'acc': base_acc_hist}, 'pfn': {'loss': pfn_loss, 'acc': pfn_acc_hist}}, f)\n    \n    try:\n        import matplotlib.pyplot as plt\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n        eps = range(1, config['epochs'] + 1)\n        ax1.plot(eps, base_loss, 'o-', label='Baseline'); ax1.plot(eps, pfn_loss, 's-', label='PFN')\n        ax1.set_xlabel('Epoch'); ax1.set_ylabel('Loss'); ax1.legend(); ax1.grid(alpha=0.3)\n        ax2.plot(eps, base_acc_hist, 'o-', label='Baseline'); ax2.plot(eps, pfn_acc_hist, 's-', label='PFN')\n        ax2.set_xlabel('Epoch'); ax2.set_ylabel('Accuracy'); ax2.legend(); ax2.grid(alpha=0.3)\n        plt.tight_layout(); plt.savefig(os.path.join(results_dir, name, 'curves.png'), dpi=150); plt.close()\n    except: pass\n    \n    return {'name': name, 'baseline': base_acc, 'pfn': pfn_acc, 'improvement': pfn_acc - base_acc}\n\n\ndef run_experiment(dataset='mnist', force_cpu=False, scenarios=None,\n                    epochs_override=None, batch_override=None):\n    device = get_device(force_cpu)\n    cfg = CONFIG[dataset]\n    epochs = epochs_override if epochs_override is not None else cfg['epochs_list'][0]\n    batch = batch_override if batch_override is not None else cfg['batches_list'][0]\n    \n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    results_dir = f'./results_{timestamp}/{dataset}_e{epochs}_b{batch}'\n    os.makedirs(results_dir, exist_ok=True)\n    \n    print(\"=\" * 60)\n    print(f\"  PFN EXPERIMENT - {dataset.upper()}\")\n    print(f\"  Device: {device} | Epochs: {epochs} | Batch: {batch}\")\n    print(\"=\" * 60)\n    \n    scenario_configs = cfg['scenarios']\n    if scenarios:\n        scenario_configs = [s for s in scenario_configs if s['name'] in scenarios or any(kw in s['name'].lower() for kw in scenarios)]\n    \n    results = []\n    for i, scenario_cfg in enumerate(scenario_configs):\n        print(f\"\\n[{i+1}/{len(scenario_configs)}] {scenario_cfg['name']}\")\n        print(\"-\" * 40)\n        results.append(run_scenario(dataset, scenario_cfg, epochs, batch, device, results_dir))\n        print(f\"  >> Base={results[-1]['baseline']:.4f} | PFN={results[-1]['pfn']:.4f} | Δ={results[-1]['improvement']:+.4f}\")\n    \n    print(\"\\n\" + \"=\" * 60)\n    wins = sum(1 for r in results if r['improvement'] > 0)\n    avg = sum(r['improvement'] for r in results) / len(results) if results else 0\n    for r in results:\n        print(f\"{r['name']:<20} {r['baseline']:.4f}  {r['pfn']:.4f}  {r['improvement']:+.4f} {'✓' if r['improvement'] > 0 else '✗'}\")\n    print(\"-\" * 60)\n    print(f\"AVG: {avg:+.4f}  ({wins}/{len(results)} wins)\")\n    \n    with open(os.path.join(results_dir, 'summary.json'), 'w') as f:\n        json.dump({'results': results, 'avg': avg, 'wins': wins}, f, indent=2)\n    \n    print(f\"\\nSaved: {results_dir}/\")\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='PFN Experiment Runner')\n    parser.add_argument('--dataset', default=None, choices=['mnist', 'cifar10', 'cifar100'],\n                        help='Dataset to use')\n    parser.add_argument('--datasets', nargs='+', default=None, help='Multiple datasets')\n    parser.add_argument('--cpu', action='store_true', help='Force CPU')\n    parser.add_argument('--scenarios', nargs='+', default=None, help='Specific scenarios')\n    parser.add_argument('--no-cache', action='store_true', help='Ignore cache')\n    parser.add_argument('--epochs', type=int, default=None, help='Override epochs')\n    parser.add_argument('--batch', type=int, default=None, help='Override batch size')\n    parser.add_argument('--epochs-list', nargs='+', type=int, default=None)\n    parser.add_argument('--batches-list', nargs='+', type=int, default=None)\n    \n    args = parser.parse_args()\n    \n    if args.no_cache:\n        baseline_cache.cache_dir = './results-baseline-nocache'\n        os.makedirs(baseline_cache.cache_dir, exist_ok=True)\n    \n    if args.datasets:\n        targets = args.datasets\n    elif args.dataset:\n        targets = [args.dataset]\n    else:\n        targets = list(CONFIG.keys())\n    \n    for ds in targets:\n        cfg = CONFIG[ds]\n        epochs_list = args.epochs_list if args.epochs_list is not None else ([args.epochs] if args.epochs is not None else cfg['epochs_list'])\n        batches_list = args.batches_list if args.batches_list is not None else ([args.batch] if args.batch is not None else cfg['batches_list'])\n        \n        for ep in epochs_list:\n            for bs in batches_list:\n                run_experiment(ds, args.cpu, args.scenarios, epochs_override=ep, batch_override=bs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T06:19:09.936061Z","iopub.execute_input":"2026-01-05T06:19:09.936518Z","iopub.status.idle":"2026-01-05T06:19:09.947917Z","shell.execute_reply.started":"2026-01-05T06:19:09.936497Z","shell.execute_reply":"2026-01-05T06:19:09.947197Z"}},"outputs":[{"name":"stdout","text":"Overwriting run_experiment.py\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"!python run_experiment.py --dataset cifar100","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T06:19:09.949126Z","iopub.execute_input":"2026-01-05T06:19:09.949357Z"}},"outputs":[{"name":"stdout","text":"============================================================\n  PFN EXPERIMENT - CIFAR100\n  Device: cuda | Epochs: 100 | Batch: 1024\n============================================================\n\n[1/3] 1.Bottleneck\n----------------------------------------\n  [Baseline] (训练)\n    [ 1/100] loss=4.5802 acc=0.0221                                             \n    [ 2/100] loss=4.4725 acc=0.0270                                             \n    [ 3/100] loss=4.4033 acc=0.0309                                             \n    [ 4/100] loss=4.3474 acc=0.0410                                             \n    [ 5/100] loss=4.2975 acc=0.0465                                             \n    [ 6/100] loss=4.2515 acc=0.0452                                             \n    [ 7/100] loss=4.2090 acc=0.0520                                             \n    [ 8/100] loss=4.1717 acc=0.0571                                             \n    [ 9/100] loss=4.1417 acc=0.0638                                             \n    [10/100] loss=4.1147 acc=0.0640                                             \n    [11/100] loss=4.0909 acc=0.0700                                             \n    [12/100] loss=4.0702 acc=0.0749                                             \n    [13/100] loss=4.0527 acc=0.0745                                             \n    [14/100] loss=4.0356 acc=0.0750                                             \n    [15/100] loss=4.0225 acc=0.0800                                             \n    [16/100] loss=4.0048 acc=0.0827                                             \n    [17/100] loss=3.9875 acc=0.0876                                             \n    [18/100] loss=3.9763 acc=0.0868                                             \n    [19/100] loss=3.9662 acc=0.0897                                             \n    [20/100] loss=3.9548 acc=0.0912                                             \n    [21/100] loss=3.9442 acc=0.0931                                             \n    [22/100] loss=3.9358 acc=0.0967                                             \n    [23/100] loss=3.9250 acc=0.0961                                             \n    [24/100] loss=3.9144 acc=0.1010                                             \n    [25/100] loss=3.9054 acc=0.1013                                             \n    [26/100] loss=3.8947 acc=0.1045                                             \n    [27/100] loss=3.8864 acc=0.1069                                             \n    [28/100] loss=3.8766 acc=0.1055                                             \n    [29/100] loss=3.8680 acc=0.1054                                             \n    [30/100] loss=3.8624 acc=0.1078                                             \n    [31/100] loss=3.8533 acc=0.1127                                             \n    [32/100] loss=3.8442 acc=0.1130                                             \n    [33/100] loss=3.8417 acc=0.1109                                             \n    [34/100] loss=3.8289 acc=0.1200                                             \n    [35/100] loss=3.8227 acc=0.1179                                             \n    [36/100] loss=3.8203 acc=0.1201                                             \n    [37/100] loss=3.8106 acc=0.1195                                             \n    [38/100] loss=3.7997 acc=0.1226                                             \n    [39/100] loss=3.7984 acc=0.1214                                             \n    [40/100] loss=3.7936 acc=0.1248                                             \n    [41/100] loss=3.7857 acc=0.1270                                             \n    [42/100] loss=3.7789 acc=0.1297                                             \n    [43/100] loss=3.7736 acc=0.1278                                             \n    [44/100] loss=3.7707 acc=0.1293                                             \n    [45/100] loss=3.7689 acc=0.1290                                             \n    [46/100] loss=3.7624 acc=0.1312                                             \n    [47/100] loss=3.7558 acc=0.1309                                             \n    [48/100] loss=3.7504 acc=0.1340                                             \n    [49/100] loss=3.7456 acc=0.1376                                             \n    [50/100] loss=3.7433 acc=0.1357                                             \n    [51/100] loss=3.7371 acc=0.1381                                             \n    [52/100] loss=3.7349 acc=0.1365                                             \n    [53/100] loss=3.7285 acc=0.1387                                             \n    [54/100] loss=3.7261 acc=0.1403                                             \n    [55/100] loss=3.7199 acc=0.1407                                             \n    [56/100] loss=3.7229 acc=0.1412                                             \n    [57/100] loss=3.7162 acc=0.1392                                             \n    [58/100] loss=3.7151 acc=0.1434                                             \n    [59/100] loss=3.7133 acc=0.1435                                             \n    [60/100] loss=3.7078 acc=0.1417                                             \n    [61/100] loss=3.7057 acc=0.1453                                             \n    [62/100] loss=3.7011 acc=0.1466                                             \n    [63/100] loss=3.6984 acc=0.1469                                             \n    [64/100] loss=3.7003 acc=0.1476                                             \n    [65/100] loss=3.6904 acc=0.1478                                             \n    [66/100] loss=3.6956 acc=0.1469                                             \n    [67/100] loss=3.6910 acc=0.1492                                             \n    [68/100] loss=3.6852 acc=0.1462                                             \n    [69/100] loss=3.6873 acc=0.1501                                             \n    [70/100] loss=3.6862 acc=0.1486                                             \n    [71/100] loss=3.6804 acc=0.1506                                             \n    [72/100] loss=3.6836 acc=0.1498                                             \n    [73/100] loss=3.6774 acc=0.1495                                             \n    [74/100] loss=3.6785 acc=0.1496                                             \n    [75/100] loss=3.6753 acc=0.1512                                             \n    [76/100] loss=3.6741 acc=0.1510                                             \n    [77/100] loss=3.6742 acc=0.1523                                             \n    [78/100] loss=3.6710 acc=0.1534                                             \n    [79/100] loss=3.6697 acc=0.1534                                             \n    [80/100] loss=3.6701 acc=0.1525                                             \n    [81/100] loss=3.6691 acc=0.1529                                             \n    [82/100] loss=3.6650 acc=0.1537                                             \n    [83/100] loss=3.6683 acc=0.1529                                             \n    [84/100] loss=3.6671 acc=0.1548                                             \n    [85/100] loss=3.6658 acc=0.1548                                             \n    [86/100] loss=3.6643 acc=0.1537                                             \n    [87/100] loss=3.6622 acc=0.1542                                             \n    [88/100] loss=3.6636 acc=0.1537                                             \n    [89/100] loss=3.6607 acc=0.1555                                             \n    [90/100] loss=3.6594 acc=0.1551                                             \n    [91/100] loss=3.6602 acc=0.1549                                             \n    [92/100] loss=3.6607 acc=0.1544                                             \n    [93/100] loss=3.6593 acc=0.1548                                             \n    [94/100] loss=3.6601 acc=0.1548                                             \n    [95/100] loss=3.6616 acc=0.1544                                             \n    [96/100] loss=3.6594 acc=0.1551                                             \n    [97/100] loss=3.6588 acc=0.1551                                             \n    [98/100] loss=3.6584 acc=0.1549                                             \n    [99/100] loss=3.6611 acc=0.1549                                             \n    [100/100] loss=3.6601 acc=0.1549                                            \n  [PFN]\n    [ 1/100] loss=4.5886 acc=0.0125                                             \n    [ 2/100] loss=4.5284 acc=0.0323                                             \n    [ 3/100] loss=4.4586 acc=0.0418                                             \n    [ 4/100] loss=4.3320 acc=0.0590                                             \n    [ 5/100] loss=4.1801 acc=0.0764                                             \n    [ 6/100] loss=4.0812 acc=0.0840                                             \n    [ 7/100] loss=4.0265 acc=0.0977                                             \n    [ 8/100] loss=3.9861 acc=0.0963                                             \n    [ 9/100] loss=3.9557 acc=0.1069                                             \n    [10/100] loss=3.9196 acc=0.1083                                             \n    [11/100] loss=3.8911 acc=0.1192                                             \n    train:  82%|██████████████████████████      | 40/49 [00:07<00:01,  5.04it/s]","output_type":"stream"}],"execution_count":null}]}